from app.core.celery_app import celery_app
from app.utils.text_utils import (
    extract_text_from_file,
    split_text_into_chunks,
    create_schema_embedding_text,
    is_excel_file,
)
from app.utils.embeddings import get_embedding
from app.vector_store.chrome_store import add_to_vector_db, add_excel_schema
from app.vector_store.sqlite_store import sqlite_store
from app.db.postgres.sync_database import SyncSessionLocal
from app.models.files import UploadedFiles
from datetime import datetime, timezone
import os
import uuid


@celery_app.task
def process_file_task(file_path: str, user_id: str):
    """Background task for processing uploaded files."""

    db = SyncSessionLocal()

    try:
        file_name = os.path.basename(file_path)
        doc_id = f"{file_name}:{uuid.uuid4().hex}"

        print(f"ğŸš€ Background task started for: {file_name} (user: {user_id})")

        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # CHECK FILE TYPE AND PROCESS ACCORDINGLY
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

        if is_excel_file(file_path):
            # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
            # EXCEL FILE: Use SQLite + Schema Embedding approach
            # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
            _process_excel_file(file_path, user_id, doc_id, file_name)
        else:
            # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
            # OTHER FILES: Use original chunking + embedding approach
            # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
            _process_document_file(file_path, user_id, doc_id, file_name)

        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # RECORD IN DATABASE
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        uploaded_record = UploadedFiles(
            user_id=user_id,
            original_filename=file_name,
            unique_filename=doc_id,
        )
        db.add(uploaded_record)
        db.commit()
        db.refresh(uploaded_record)

        print(
            f"âœ… Successfully processed: {file_name} (record id: {uploaded_record.id})"
        )

    except Exception as e:
        db.rollback()
        print(f"âŒ Error processing {file_path}: {str(e)}")
        raise

    finally:
        db.close()
        print("ğŸ”’ DB connection closed")


def _process_excel_file(file_path: str, user_id: str, doc_id: str, file_name: str):
    """
    Process Excel file using SQLite approach.

    1. Store actual DATA in SQLite (for SQL queries)
    2. Store SCHEMA in ChromaDB (for finding relevant tables)
    """
    print(f"ğŸ“Š Processing Excel file: {file_name}")

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # STEP 1: Import data into SQLite
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print("  â†’ Importing data to SQLite...")
    created_tables = sqlite_store.import_excel(
        file_path=file_path, user_id=user_id, doc_id=doc_id
    )
    print(f"  âœ“ Created {len(created_tables)} table(s) in SQLite")

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # STEP 2: Create embeddings for SCHEMAS only
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print("  â†’ Creating schema embeddings...")

    for schema_info in created_tables:
        # Create text for embedding (schema description, not data!)
        schema_text = create_schema_embedding_text(schema_info)

        # Generate embedding for schema
        schema_embedding = get_embedding(schema_text)

        # Store in ChromaDB
        add_excel_schema(
            schema_info=schema_info, embedding=schema_embedding, schema_text=schema_text
        )

        print(
            f"    âœ“ Schema embedded: {schema_info['table_name']} "
            f"({schema_info['row_count']} rows, "
            f"{len(schema_info['columns'])} columns)"
        )

    print(f"  âœ“ Excel processing complete")


def _process_document_file(file_path: str, user_id: str, doc_id: str, file_name: str):
    """
    Process non-Excel files using original chunking approach.
    (PDFs, Word docs, text files, etc.)
    """
    print(f"ğŸ“„ Processing document file: {file_name}")

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # STEP 1: Extract text
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print("  â†’ Extracting text...")
    text = extract_text_from_file(file_path)
    print(f"  âœ“ Extracted {len(text)} characters")

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # STEP 2: Split into chunks
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print("  â†’ Splitting into chunks...")
    chunks = split_text_into_chunks(text)
    print(f"  âœ“ Created {len(chunks)} chunks")

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # STEP 3: Generate embeddings
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print("  â†’ Generating embeddings...")
    embeddings = [get_embedding(chunk) for chunk in chunks]
    print(f"  âœ“ Generated {len(embeddings)} embeddings")

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # STEP 4: Store in ChromaDB
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    now_iso = datetime.now(timezone.utc).isoformat()

    metadatas = [
        {
            "file_name": file_name,
            "chunk_index": i,
            "upload_time": now_iso,
            "source_path": file_path,
            "doc_id": doc_id,
            "user_id": str(user_id),
            "file_type": "document",
        }
        for i in range(len(chunks))
    ]

    ids = [f"{doc_id}:{i}" for i in range(len(chunks))]

    print("  â†’ Adding to vector DB...")
    add_to_vector_db(chunks, embeddings, metadatas, ids)
    print(f"  âœ“ Document processing complete")
